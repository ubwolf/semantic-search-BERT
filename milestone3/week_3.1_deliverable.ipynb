{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcd4c16969b4374ae0610a96236c120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.554148 \t answer: 75â€“200 million\n",
      "score: 0.000126 \t answer: 15%.\n",
      "score: 0.000045 \t answer: between 17 million and 50 million,\n",
      "score: 0.000001 \t answer: 37.9 million\n",
      "score: 0.000000 \t answer: pandemic prevention in some respects.\n"
     ]
    }
   ],
   "source": [
    "import faiss \n",
    "import json\n",
    "import torch \n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        with open(self.path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "\n",
    "    def save_data(self, data ,save_path):\n",
    "        with open(save_path, 'w') as file:\n",
    "            json.dump(data, file)\n",
    "\n",
    "\n",
    "class TransformerEncoder:\n",
    "\n",
    "\n",
    "    def __init__(self, model_name, pipe_model, token_model):\n",
    "        self.encoder = SentenceTransformer(model_name)\n",
    "        self.QnA = pipeline('question-answering', model=pipe_model,\n",
    "                            tokenizer=token_model)\n",
    "\n",
    "\n",
    "    def find_answers(self, text, query, index, k=5):\n",
    "        query_encoded = self.encoder.encode([query],convert_to_tensor=True,\n",
    "                                            show_progress_bar=True).cpu().detach().numpy()\n",
    "        D, I = index.search(query_encoded, k)\n",
    "        answers = [self.QnA({'question': query,\n",
    "                             'context': f'{text[i]}'}) for i in I[0]]\n",
    "        return sorted(answers, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    \n",
    "    def print_answers(self, answers):\n",
    "        for answer in answers:\n",
    "            print(f\"score: {answer['score']:.6f} \\t answer: {answer['answer']}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = DataLoader('../data_hub/data_2.json').load_data()\n",
    "    embedder = TransformerEncoder('distilbert-base-nli-stsb-mean-tokens',\n",
    "                                  'ktrapeznikov/albert-xlarge-v2-squad-v2',\n",
    "                                   'albert-xlarge-v2')\n",
    "    index = faiss.read_index('../data_hub/faiss.index')\n",
    "    \n",
    "    query = \"How many people have died during Black Death?\"\n",
    "    text_corpus = [d['text'] for d in data]\n",
    "    answers = embedder.find_answers(text_corpus, query, index)\n",
    "    embedder.print_answers(answers)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92b3875b50552a19f5c8b904a9bd9455a012f7f4a6644b61f5845177f2c2dbee"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('semantic_searchenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
